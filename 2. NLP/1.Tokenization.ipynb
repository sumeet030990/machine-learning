{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus:str = \"Tokenization in natural language processing (NLP) is the process of breaking text into smaller units, or tokens, that are easier for machines to understand. Tokens can be words, characters, phrases, or sentences.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization\n",
    "- from corpus(paragraph) -> document(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization in natural language processing (NLP) is the process of breaking text into smaller units, or tokens, that are easier for machines to understand.',\n",
       " 'Tokens can be words, characters, phrases, or sentences.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "documents = sent_tokenize(corpus)\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization\n",
    "- from corpus(paragraph) -> words\n",
    "- from document(sentences) -> words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'the', 'process', 'of', 'breaking', 'text', 'into', 'smaller', 'units', ',', 'or', 'tokens', ',', 'that', 'are', 'easier', 'for', 'machines', 'to', 'understand', '.', 'Tokens', 'can', 'be', 'words', ',', 'characters', ',', 'phrases', ',', 'or', 'sentences', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = word_tokenize(corpus)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'the', 'process', 'of', 'breaking', 'text', 'into', 'smaller', 'units', ',', 'or', 'tokens', ',', 'that', 'are', 'easier', 'for', 'machines', 'to', 'understand', '.']\n",
      "['Tokens', 'can', 'be', 'words', ',', 'characters', ',', 'phrases', ',', 'or', 'sentences', '.']\n"
     ]
    }
   ],
   "source": [
    "for document in documents:\n",
    "  print(word_tokenize(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to seprate word with punctuations also i.e. all symbols like !, \", ' and , will be treated as seprate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "word_with_punctuation = wordpunct_tokenize(corpus)\n",
    "print(word_with_punctuation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
